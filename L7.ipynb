{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inteligencia Artificial - Laboratorio 7 -\n",
    "- Jimena Hernández 21199\n",
    "- Mark Albrand 21004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Teoría\n",
    "\n",
    "- ¿Qué es el temporal difference learning y en qué se diferencia de los métodos tradicionales de aprendizaje supervisado? Explique el concepto de \"error de diferencia temporal\" y su papel en los algoritmos de aprendizaje por refuerzo\n",
    "\n",
    "    -  El TD learning combina elementos del aprendizaje supervisado y el no supervisado, se diferencia de los métodos tradicionales de aprendizaje supervisado ya que en el aprendizaje supervisado, el modelo aprende a partir de un conjunto de datos que incluye tanto las entradas como las salidas deseadas. En cambio el TD learning se basa en la idea de aprender directamente de la experiencia, sin necesidad de conocer la respuesta correcta de antemano.\n",
    "\n",
    "| Característica              | TD learning                                                           | Aprendizaje Supervisado                            |\n",
    "|-----------------------------|-----------------------------------------------------------------------|----------------------------------------------------|\n",
    "| **Fuente de los datos**     | Basado en la experiencia y las recompensas acumuladas                 | Conjunto de datos etiquetados con entradas/salidas |\n",
    "| **Objetivo de aprendizaje** | Maximizar las recompensas futuras                                     | Minimizar el error entre predicciones y etiquetas  |\n",
    "| **Cálculo del error**       | Error de diferencia temporal entre recompensas predichas y observadas | Diferencia entre la predicción y la etiqueta real  |\n",
    "| **Tipo de entorno**         | Entornos dinámicos y donde la información puede ser incompleta        | Entornos controlados con datos completos           |\n",
    "\n",
    "El \"error de diferencia temporal\" se calcula como la diferencia entre las recompensas predichas y las observadas, ajustándose en función de las recompensas futuras anticipadas. ```https://web.stanford.edu/group/pdplab/pdphandbook/handbookch10.html```\n",
    "\n",
    "- En el contexto de los juegos simultáneos, ¿cómo toman decisiones los jugadores sin conocer las acciones de sus oponentes? De un ejemplo de un escenario del mundo real que pueda modelarse como un juego simultáneo y discuta las estrategias que los jugadores podrían emplear en tal situación.\n",
    "    - En el contexto de los juegos simultáneos, los jugadores toman decisiones sin conocer las acciones de sus oponentes mediante la evaluación de las posibles estrategias que podrían seguir sus oponentes y seleccionando la estrategia que maximice su propia utilidad, asumiendo que los oponentes también están seleccionando sus estrategias de manera óptima. Un buen ejemplo de un juego simultáneo puede ser 'piedras papel o tijeras', en el que los jugadores deben seleccionar una de las tres opciones sin conocer la elección de su oponente. En este caso, una estrategia óptima sería seleccionar una opción al azar, ya que no hay una estrategia que garantice una victoria. Otra estrategia popular es elegir siempre piedra, ya que hay una teoría que dice que las personas eligen la última opción que escucharon, que en este juego suele ser tijeras, por lo que elegir piedra podría ser una estrategia ganadora contra ciertos oponentes \n",
    "\n",
    "- ¿Qué distingue los juegos de suma cero de los juegos de suma cero y cómo afecta esta diferencia al proceso de toma de decisiones de los jugadores? Proporcione al menos un ejemplo de juegos que entren en la categoría de juegos de no suma cero y discuta las consideraciones estratégicas únicas involucradas\n",
    "\n",
    "    - Suma cero: Se caracterizan por tener un balance en el que la ganancia de un jugador implica una pérdida equivalente para el otro, de manera que la suma total de ganancias y pérdidas en el juego es cero.\n",
    "    - No suma cero: permiten situaciones en las que todos los jugadores pueden beneficiarse o perder al mismo tiempo, en donde la suma de ganancias y pérdidas puede ser mayor o menor que cero. \n",
    "\n",
    "    Ejemplo:\"Dilema del Priosionero\". El fiscal pregunta a A y B individualmente si cada uno testificará contra el otro.\n",
    "    ● Si ambos testifican, ambos serán condenados a 5 años\n",
    "    de cárcel.\n",
    "    ● Si ambos se niegan, ambos serán condenados a 1 año\n",
    "    de cárcel.\n",
    "    ● Si sólo uno testifica, sale libre; el otro recibe una sentencia\n",
    "    de 10 años.\n",
    "\n",
    "    Los jugadores deben considerar no solo sus propias preferencias y las consecuencias de sus acciones, sino también cómo sus decisiones afectarán y serán afectadas por las decisiones de los otros jugadores.\n",
    "    \n",
    "    ```Suriano, A. (2024). Juegos Parte 2: Inteligencia Artificial. Departamento de Ciencias de la Computación y Tecnologías de la Información, Universidad del Valle de Guatemala.```\n",
    "\n",
    "- ¿Cómo se aplica el concepto de equilibrio de Nash a los juegos simultáneos? Explicar cómo el equilibrio de Nash representa una solución estable en la que ningún jugador tiene un incentivo para desviarse unilateralmente de la estrategia elegida.\n",
    "\n",
    "    - Este concepto se aplica ya que el equilibrio de Nash basicamente dice que cuando llegamos a la estrategia óptima, no hay ninguna otra que supere esta, es decir, es un estado de la interacción entre jugadores donde la decisión de cada uno es óptima, considerando las decisiones tomadas por los demás. La existencia del equilibrio de Nash en juegos con un número finito de estrategias y jugadores está garantizada por el teorema de existencia de Nash, el cual asegura que siempre habrá al menos un equilibrio de Nash, posiblemente involucrando estrategias mixtas, donde los jugadores eligen entre varias estrategias posibles con ciertas probabilidades​. <br><br>\n",
    "    Una vez que se alcanza un equilibrio de Nash en un juego, ningún jugador tiene un incentivo para cambiar su curso de acción dado que cualquier desviación resultaría en una situación menos preferible para él.\n",
    "    Un ejemplo de esto, sucede en el ejemplo anterior del dilema del prisionero, el equilibrio de Nash en este juego se alcanza cuando ambos deciden confesar, ya que confesar es la estrategia óptima para cada uno, asumiendo que el otro también confiesa. \n",
    "    ```Suriano, A. (2024). Juegos Parte 2: Inteligencia Artificial. Departamento de Ciencias de la Computación y Tecnologías de la Información, Universidad del Valle de Guatemala.```\n",
    "\n",
    "- Discuta la aplicación del temporal difference learning en el modelado y optimización de procesos de toma de decisiones en entornos dinámicos. ¿Cómo maneja el temporal difference learning el equilibrio entre exploración y explotación y cuáles son algunos de los desafíos asociados con su implementación en la \n",
    "práctica?\n",
    "    - El TD learning es un enfoque de aprendizaje por refuerzo que se basa en la idea de aprender directamente de la experiencia, sin necesidad de conocer la respuesta correcta de antemano. En el contexto de la toma de decisiones en entornos dinámicos, el TD learning es útil para modelar y optimizar procesos de toma de decisiones, gracias a que a los agentes aprenden a partir de la experiencia y las recompensas acumuladas, ajustando sus acciones en base a esto. El TD learning maneja el equilibrio entre exploración y explotación mediante el uso de una política de exploración que permite a los agentes explorar nuevas acciones y estados, mientras que también utiliza una política de explotación que permite a los agentes aprovechar las acciones y estados conocidos. Algunos de los desafíos asociados con la implementación del TD learning en la práctica incluyen la selección de la tasa de aprendizaje adecuada, la gestión de la exploración y la explotación, y la selección de recompensas que ayuden a modelar correctamente el problema.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
